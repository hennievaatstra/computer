Fluentd+ELK alternative to Splunk on Centos8
=============================================

source: https://docs.fluentd.org/how-to-guides/free-alternative-to-splunk-by-fluentd

. kickstart minimal CentOS8

. install java 8
	# dnf install java-1.8.0-openjdk.x86_64

. add account
	# useradd -m fluentd
	# passwd fluentd
	:enable sudo for the installation

. install Elasticsearch
	# su - fluentd
	$ curl -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.1.0.tar.gz
	$ tar xf ./elasticsearch-6.1.0.tar.gz
		.after installation start Elasticsearch by running:
		$ cd elasticsearch-6.1.0
		$ ./bin/elasticsearch

. install Kibana
	$ curl -O https://artifacts.elastic.co/downloads/kibana/kibana-6.1.0-linux-x86_64.tar.gz
	$ tar -xf kibana-6.1.0-linux-x86_64.tar.gz
		. configure in 
		config/kibana.yml
		  server.host: crdlxhv02.son.catena.nl
		. once installation is complete start Kibana by running:
		$ cd tar -xf kibana-6.1.0-linux-x86_64
		$  ./bin/kibana
		then access http://localhost:5601

. install Fluentd
	$ curl -O https://toolbelt.treasuredata.com/sh/install-redhat-td-agent4.sh	
	$ bash install-redhat-td-agent4.sh

. enable and start td-agent
	$ sudo systemctl enable td-agent

. install elasticsearch plugin for fluentd
	$ sudo /usr/sbin/td-agent-gem install fluent-plugin-elasticsearch --no-document
Fetching fluent-plugin-elasticsearch-4.2.2.gem
Successfully installed fluent-plugin-elasticsearch-4.2.2
1 gem installed

. configure td-agent to interface properly with Elasticsearch
	modify /etc/td-agent/td-agent.conf as shown below:

# get logs from syslog
<source>
  @type syslog
  port 42185
  tag syslog
</source>

# get logs from fluent-logger, fluent-cat or other fluentd instances
<source>
  @type forward
</source>

<match syslog.**>
  @type elasticsearch
  logstash_format true
  <buffer>
    flush_interval 10s # for testing
  </buffer>
</match>

. start td-agent
	# systemctl start td-agent

. set up rsyslogd
	:forward logging to fluentd, add to /etc/rsyslog.conf
	*.* @127.0.0.1:42185
	:use @ for  UDP
	:use @@ for TCP
	:using hostname on remote nodes didn't seem to work
	: set to: *.* @10.128.16.49:42185

. restart rsyslog
	# systemctl restart rsyslog

. create systemd service files for Elasticsearch and Kibana
	# vi /etc/systemd/system/elasticsearch.service
[Unit]
Description=Elasticsearch
Requires=network-online.target
After=network-online.target
Wants=network-online.target
[Service]
Environment=ES_HOME=/home/fluent/elasticsearch-6.1.0
WorkingDirectory=/home/fluent/elasticsearch-6.1.0
PIDFile=/run/elasticsearch.pid
ExecStart=/home/fluent/elasticsearch-6.1.0/bin/elasticsearch -p ${PIDFile} --quiet
ExecReload=/usr/bin/kill -HUP $MAINPID
TimeoutSec=180
User=fluent
Group=fluent
[Install]
WantedBy=multi-user.target

	# vi /etc/systemd/system/kibana.service
[Unit]
Description=Kibana service
Requires=network.target
After=network.target
Wants=network-online.target
[Service]
PIDFile=/run/kibana.pid
ExecStart=/home/fluent/kibana-6.1.0-linux-x86_64/bin/kibana -Q
ExecReload=/usr/bin/kill -HUP $MAINPID
TimeoutSec=180
User=fluent
Group=fluent
[Install]
WantedBy=multi-user.target

	# systemctl daemon-reload

. now start Elasticsearch and Kibana
	# su - fluentd
	$ sudo systemctl start elasticsearch
	$ sudo systemctl start kibana

	old, in interactive shell:
		$ cd elasticsearch-6.1.0
		$ nohup bin/elasticsearch &
		$ cd ../kibana-6.1.0-linux-x86_64
		$ bin/kibana 
		<Ctrl>+Z
		$ bg %1

. store and search event patterns
	http://crdlxhv02.son.catena.nl:5601

	> click 'Set up index patterns'
	use logstash-* as the index pattern
	> Next
	select @timestamp as the time-filter field
	< Create index pattern

	> click 'Discover' [left side column]

. test an incoming message
	# logger -t test foobar
	# tail -1 /var/log/messages 
Nov  4 15:55:14 crdlxhv02 test[25819]: foobar

	. refresh webpage
November 4th 2020, 15:55:14.000	host:crdlxhv02 ident:test pid:25819 message:foobar @timestamp:November 4th 2020, 15:55:14.000 _id:hTXBk3UBOO4LlEMjYGNa _type:fluentd _index:logstash-2020.11.04 _score: -

---

Install filebeat onto kibana+ELK
================================

. download & install filebeat
	# su - fluent
	$ curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.5.4-x86_64.rpm
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 10.9M  100 10.9M    0     0  7403k      0  0:00:01  0:00:01 --:--:-- 7403k

	$ sudo rpm -ivh filebeat-6.5.4-x86_64.rpm 
warning: filebeat-6.5.4-x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID d88e42b4: NOKEY
Verifying...                          ################################# [100%]
Preparing...                          ################################# [100%]
Updating / installing...
   1:filebeat-6.5.4-1                 ################################# [100%]

. configure the filebeat
	$ cd /etc/filebeat
	$ sudo vi filebeat.yml, set
  - enabled: true
  paths:
    - /var/log/messages

. start &enable filebeat
	$ sudo systemctl enable --now filebeat

. ues it in Kibana
	> Kibana > Set up index patterns > 
	@Index pattern: filebeat-* > Next step
	@Time Filter field name > @timestamp > Create index pattern
