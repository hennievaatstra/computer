Install docker-kubernetes cluster on three CentOS8 mochines
===========================================================

:-----> install Kubernetes MASTER

. kickstart minimal CentOS8

. disable SElinux
	# setenforce 0
	# vi /etc/sysconfig/selinux, set
SELINUX=disabled

. configure firewall [if FirewallD is running]
	# firewall-cmd --permanent --add-port=6443/tcp
	# firewall-cmd --permanent --add-port=2379-2380/tcp
	# firewall-cmd --permanent --add-port=10250/tcp
	# firewall-cmd --permanent --add-port=10251/tcp
	# firewall-cmd --permanent --add-port=10252/tcp
	# firewall-cmd --permanent --add-port=10255/tcp
	# firewall-cmd --reload

	:if NO firewall, only run following commands
	# modprobe br_netfilter
	# echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables

. install Docker-CE 
	:add repo
	# dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo
	:install containerd.io as prereq
	# dnf install https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm
	# dnf install docker-ce
	# systemctl enable --now docker

. install Kubernetes
	# vi /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg

	# dnf install kubeadm -y

. create a 'control-plane' master
	: swap MUST be disabled
	# swapoff -a
	# vi /etc/fstab, comment swap entry

. start kubelet
	# systemctl enable --now kubelet

	# kubeadm init
	: save the line with 'kubeadm join' to file!
	: needed to join worker nodes

. enable a user to start the cluster [root in this case]
	. to use a sudo enabled user, run:
	# useradd -m kubeman
	# visudo, add
	 kubeman ALL=(ALL)       NOPASSWD: ALL

	# su - kubeman
	$ mkdir -p $HOME/.kube
	$ cd .kube
	$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

. confirm the 'kubectl command' is activated
	$ kubectl get nodes
NAME        STATUS     ROLES    AGE    VERSION
crdlxhv01   NotReady   master   8m5s   v1.18.3

. setup the pod network
	:Deploying the network cluster is a highly flexible process depending on your needs and there are many options available. Since we want to keep our installation as simple as possible, we will use Weavenet plugin which does not require any configuration or extra code and it provides one IP address per pod which is great for us. 

	$ export kubever=$(kubectl version | base64 | tr -d '\n')
	$ kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$kubever"

	$ kubectl get nodes
	: it may [still] take some time before we get status 'Ready', just wait a bit
$ kubectl get nodes
NAME        STATUS     ROLES    AGE   VERSION
crdlxhv01   NotReady   master   10m   v1.18.3

--------------------------------------------------------------------------------
:-----> add WORKER NODE to the cluster

. kickstart minimal CentOS8

. disable SElinux
	# setenforce 0
	# vi /etc/sysconfig/selinux, set
SELINUX=disbabled

. configure firewall [if FirewallD is running]
	# firewall-cmd --permanent --add-port=6443/tcp
	# firewall-cmd --permanent --add-port=2379-2380/tcp
	# firewall-cmd --permanent --add-port=10250/tcp
	# firewall-cmd --permanent --add-port=10251/tcp
	# firewall-cmd --permanent --add-port=10252/tcp
	# firewall-cmd --permanent --add-port=10255/tcp
	# firewall-cmd --reload

	:if NO firewall, only run following commands:
	# modprobe br_netfilter
	# echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables

. install Docker-CE 
	:add repo
	# dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo
	:install containerd.io as prereq
	# dnf install https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm
	# dnf install docker-ce
	# systemctl enable --now docker

. install Kubernetes
	# vi /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg

	# dnf install kubeadm -y

. disable swap
	# swapoff -a
	# vi /etc/fstab, disable swap entry

. start kubelet
	# systemctl enable --now kubelet

. join the cluster, as root!
	: use the line from the 'kubeadm init' as ran on the master
	# kubeadm join 10.128.16.11:6443 --token e3qihn.1a106e8pd90zrtxn --discovery-token-ca-cert-hash sha256:b8fd111895d17bb0539185d29b274724c355348330a220d576b7b3a387f2e33e

	:on the master, check:
	$ kubectl get nodes
	:worker node should be listed now
	:again, it may take some time to get to status 'Ready'
NAME        STATUS     ROLES    AGE   VERSION
crdlxhv01   NotReady   master   19m   v1.18.3
crdlxhv02   NotReady   <none>   20s   v1.18.3


:-----> install 'Dashboard UI'
	:dashboard is not deployed by default

. deploy dashboard on master
	# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml
	:The UI can only be accessed from the machine where the command is executed. See kubectl proxy --help for more options.
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created

. optional install firefox on kubemaster
	# dnf install firefox xauth
	then surf to:
	http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
	:DEFAULT dashboard is only accessible on the localhost

? ============================================================================ ?
. enable access to dashboard from the network
	:query the namespace for the dashboard
	# kubectl -n kube-system get services --all-namespaces
NAMESPACE              NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
default                kubernetes                  ClusterIP   10.96.0.1        <none>        443/TCP                  104m
default                nginx                       ClusterIP   10.97.136.237    10.128.16.9   80/TCP                   9m56s
kube-system            kube-dns                    ClusterIP   10.96.0.10       <none>        53/UDP,53/TCP,9153/TCP   104m
kubernetes-dashboard   dashboard-metrics-scraper   ClusterIP   10.105.178.179   <none>        8000/TCP                 27m
kubernetes-dashboard   kubernetes-dashboard        ClusterIP   10.102.167.142   <none>        443/TCP                  27m

	# kubectl describe services dashboard-metrics-scraper --namespace=kubernetes-dashboard
Name:              dashboard-metrics-scraper
Namespace:         kubernetes-dashboard
Labels:            k8s-app=dashboard-metrics-scraper
Annotations:       Selector:  k8s-app=dashboard-metrics-scraper
Type:              ClusterIP
IP:                10.105.178.179
Port:              <unset>  8000/TCP
TargetPort:        8000/TCP
Endpoints:         10.32.0.6:8000
Session Affinity:  None
Events:            <none>

. edit settings
	# kubectl -n kubernetes-dashboard edit service kubernetes-dashboard
set:
	type: ClusterIP
	to
	type: NodePort
:wq
service/kubernetes-dashboard edited

. check for the NodePort
	# kubectl describe services kubernetes-dashboard --namespace=kubernetes-dashboard|grep NodePort
Type:                     NodePort
NodePort:                 <unset>  31447/TCP

? =========================================================================== ?

. create service account
	# vi dashboard-adminuser.yaml
apiVersion: v1
kind: ServiceAccount
metadata: 
  name: admin-user
  namespace: kubernetes-dashboard

. apply
	# kubectl apply -f dashboard-adminuser.yaml
serviceaccount/admin-user created

. create cluster role binding
	# vi ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard

. apply
	# kubectl apply -f ClusterRoleBinding
clusterrolebinding.rbac.authorization.k8s.io/admin-user created

.get 'bearer' token to log in
	# kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret|grep admin-user|awk '{print $1}')
.....
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6InhKNHFtV3cyU0JQN0doWkhYa2VudHQ4dlVqQ0twUkFPQjJqcEpTWi04amsifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi01cnNtaCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjMyZDhjMTcxLWE5NjgtNGYxMS1iMjk0LWFiYTkzOGE4NDBhMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.ko-gT3wuh3rcEOxJkBTpG_n72_Ngu6gmcH0VQKPIyqZdt3zrMKRSfrdxxziEkDi_0IHk3hMYi_XyfAd7_XL7GwyDhKaE2AJGiiBw0D0YptdR3gdBVEuAdvW746BCyOOuX2LOM5V6ZJdKCBvi2Q7wzlmFwO8CnsQm4G2_tI3ZowH2dphh9A389wy42rc1OKhPd_meaNpqpzdpHtsHQQZKT4hewU0CoqhH5XDrJNdT6LzqKIHoAs3CimONL7xSgYdaE0z6blQlzweXchvnjuzakUNAdE-ze2jEZXp5S1_pefgx90wapWu_LXfGAa8blJdIUUsWRySRqgj9bl1SxVKpLg

	:copy the token to login

. start the proxy
	:localhost
	# kubectl proxy

	? :network
	? # kubectl proxy --address='0.0.0.0' --accept-hosts='^*$'

. use browser on master to login
	http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
	select Token
? . use browser on net [laptop] to login
	https://10.128.16.9:31447

--------------------------------------------------------------------------------
:-----> Deploying to Kubernetes

. create the deployment for an nginx pod
	# kubectl run nginx --image=nginx
pod/nginx created
	# kubectl create deployment nginx --image=nginx:1.10.0
deployment.apps/nginx created
	:strictly you can create the deployment as a first step, the pod wil be created automagic

	.check
	# kubectl get deployment nginx
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
nginx   0/1     1            0           4s

	$ kubectl get deployment nginx -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR
nginx   0/1     1            0           12s   nginx        nginx:1.10.0   app=nginx

.expose the deployment
	# kubectl expose deployment nginx --external-ip=10.128.16.11 --port=80 --target-port=80
service/nginx exposed
	.check to see if service is running:
	 # kubectl get service
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP    PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1      <none>         443/TCP   5h5m
nginx        ClusterIP   10.96.219.13   10.128.16.11   80/TCP    4m31s

	--- 
	:alternatively, on a non-standard port:
	 $ kubectl expose deployment nginx01 --external-ip=10.128.16.11 --port=8008 --target-port=80
	 service/nginx01 exposed
	: this makes http port80 in the pod available through port 8008 on the external-ip [kubemaster]
	.check
	 $ kubectl get deployment -o wide
	 NAME      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES         SELECTOR
	 nginx01   1/1     1            1           3m56s   nginx        nginx:1.10.0   app=nginx01
	---

. scaling out
	:run 3 pods
	# kubectl scale deployment nginx --replicas=3
deployment.apps/nginx scaled

	# kubectl get deployment nginx
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
nginx   3/3     3            3           6m29s

	# kubectl get pods -o wide  
NAME                     READY   STATUS    RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
nginx                    1/1     Running   0          4m32s   10.44.0.1   crdlxhv01    <none>           <none>
nginx-55c6b6cb9c-2jsks   1/1     Running   0          22s     10.44.0.3   crdlxhv01    <none>           <none>
nginx-55c6b6cb9c-67kv2   1/1     Running   0          22s     10.44.0.6   crdlxhv01    <none>           <none>
nginx-55c6b6cb9c-828gm   1/1     Running   0          22s     10.44.0.4   crdlxhv01    <none>           <none>
nginx-55c6b6cb9c-856g8   1/1     Running   0          22s     10.44.0.5   crdlxhv01    <none>           <none>
nginx-55c6b6cb9c-gp6ks   1/1     Running   0          22s     10.44.0.2   crdlxhv01    <none>           <none>
nginx-55c6b6cb9c-hqx8l   1/1     Running   0          22s     10.36.0.1   crdlxvm052   <none>           <none>
nginx-55c6b6cb9c-qg922   1/1     Running   0          22s     10.36.0.4   crdlxvm052   <none>           <none>
nginx-55c6b6cb9c-slkwx   1/1     Running   0          22s     10.36.0.3   crdlxvm052   <none>           <none>
nginx-55c6b6cb9c-zchdt   1/1     Running   0          22s     10.36.0.2   crdlxvm052   <none>           <none>

	. to delete a deployment:
	 # kubectl delete deployment tecmint-web
deployment.apps "tecmint-web" deleted
	 # kubectl delete service tecmint-web
service "tecmint-web" deleted
	 # kubectl delete pod tecmint-web
pod "tecmint-web" deleted

	see also: https://kubernetes.io/docs/reference/kubectl/overview/

--------------------------------------------------------------------------------
:-----> Installing Helm
	:Helm is package manager for K8s

. get helm software
	# wget https://get.helm.sh/helm-v3.3.4-linux-amd64.tar.gz
	# tar zxvf helm-v3.3.4-linux-amd64.tar.gz linux-amd64/helm

. install
	# cp linux-amd64/helm /usr/local/bin

. initialize a helm chart repo
	# su - kubeman
	$ helm repo add stable https://kubernetes-charts.storage.googleapis.com/
"stable" has been added to your repositories

. work with helm
	$ helm repo list
NAME    URL                                              
stable  https://kubernetes-charts.storage.googleapis.com/

	$ helm search repo stable
NAME                                    CHART VERSION   APP VERSION             DESCRIPTION                                       
stable/acs-engine-autoscaler            2.2.2           2.1.1                   DEPRECATED Scales worker nodes within agent pools 
stable/aerospike                        0.3.3           v4.5.0.5                A Helm chart for Aerospike in Kubernetes          
stable/airflow                          7.10.1          1.10.12                 Airflow is a platform to programmatically autho...
..... and MANY more

. install an example chart
	$  helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "stable" chart repository
Update Complete. ?Happy Helming!?


	$ helm install stable/mysql --generate-name
NAME: mysql-1601459855
LAST DEPLOYED: Wed Sep 30 11:57:40 2020
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
MySQL can be accessed via port 3306 on the following DNS name from within your cluster:
mysql-1601459855.default.svc.cluster.local

To get your root password run:

    MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace default mysql-1601459855 -o jsonpath="{.data.mysql-root-password}" | base64 --decode; echo)

To connect to your database:

1. Run an Ubuntu pod that you can use as a client:

    kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il

2. Install the mysql client:

    $ apt-get update && apt-get install mysql-client -y

3. Connect using the mysql cli, then provide your password:
    $ mysql -h mysql-1601459855 -p

To connect to your database directly from outside the K8s cluster:
    MYSQL_HOST=127.0.0.1
    MYSQL_PORT=3306

    # Execute the following command to route the connection:
    kubectl port-forward svc/mysql-1601459855 3306

    mysql -h ${MYSQL_HOST} -P${MYSQL_PORT} -u root -p${MYSQL_ROOT_PASSWORD}

:note the pod kept 'Pending' state, probably because a persistent volume claim was in the config
